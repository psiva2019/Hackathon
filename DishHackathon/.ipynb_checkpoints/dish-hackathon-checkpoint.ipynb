{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train=pd.read_csv('/kaggle/input/dish-hackathon-2021/Train_Dataset.csv')\n",
    "#df_test=pd.read_csv('/kaggle/input/dish-hackathon-2021/Test_Dataset.csv')\n",
    "\n",
    "df_train=pd.read_csv('Train_Dataset.csv')\n",
    "df_test=pd.read_csv('Test_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the Data\n",
    "print('****Shape of the train data****')\n",
    "print('*************************')\n",
    "print('No of rows\\t:\\t{}\\nNo of columns\\t:\\t{}'.format(df_train.shape[0],df_train.shape[1]))\n",
    "\n",
    "#Shape of the Data\n",
    "print('\\n****Shape of the test data****')\n",
    "print('*************************')\n",
    "print('No of rows\\t:\\t{}\\nNo of columns\\t:\\t{}'.format(df_test.shape[0],df_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('****Show infomation of the train data****')\n",
    "print('***********************************')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('****Show infomation of the test data****')\n",
    "print('***********************************')\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvar='Default'\n",
    "perclass = df_train[tvar].value_counts(normalize=True)\n",
    "tardisval = df_train[tvar].value_counts()\n",
    "print('****Target Varible Distribution****')\n",
    "print('************************************')\n",
    "print('{} - Yes \\t:\\t {} which is {}% \\n{} - No\\t:\\t {} which is {}%'.format(tvar,tardisval[1],round(perclass[1]*100,2),tvar,tardisval[0],round(perclass[0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Distribution is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_train.isnull().sum()[df_train.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_train.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_test.isnull().sum()[df_test.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_test.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate values\n",
    "print('****Duplicates data in the train dataset without EmployeeID Column****')\n",
    "print('**********************************************************************')\n",
    "\n",
    "dups = df_train.loc[:, ~df_train.columns. isin(['ID'])].duplicated().sum()\n",
    "if dups ==0:\n",
    "    print('There is no duplicate values in the data.') \n",
    "else:\n",
    "    print('There are {} duplicates in the data which is {}% of total data'.format(dups,round((dups/df_train.shape[0])*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('****Describe the data - Continues Variables****')\n",
    "print('***********************************************')\n",
    "df_train.loc[:, ~df_train.columns. isin(['EmployeeID'])].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('****Describe the data - Catagorical Variables****')\n",
    "print('*************************************************')\n",
    "print(df_train.describe(include='object').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=[]\n",
    "num=[]\n",
    "for i in df_train.loc[:, ~df_train.columns. isin(['ID'])]:\n",
    "    if df_train[i].dtype==\"object\":\n",
    "        cat.append(i)\n",
    "    else:\n",
    "        num.append(i)\n",
    "       \n",
    "print('Catogorical Variables : \\n*****************\\n', cat) \n",
    "print('\\nNumerical Variables : \\n*****************\\n', num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique values of the catagrical variables\n",
    "for i in cat:\n",
    "    print(f'Unique values of the catagorical variable {i}')\n",
    "    print('*******************************************************\\n')\n",
    "    print(df_train[df_train[i].str.replace(\".\",\"\").str.isnumeric()==False][i].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique values of the catagrical variables\n",
    "for i in cat:\n",
    "    print(f'Unique values of the catagorical variable {i}')\n",
    "    print('*******************************************************\\n')\n",
    "    print(df_test[df_test[i].str.replace(\".\",\"\").str.isnumeric()==False][i].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Client_Income'] = df_train['Client_Income'].replace('$',np.nan).astype(float)\n",
    "df_test['Client_Income'] = df_test['Client_Income'].replace('$',np.nan).astype(float)\n",
    "\n",
    "\n",
    "df_train['Credit_Amount'] = df_train['Credit_Amount'].replace('$',np.nan).astype(float)\n",
    "df_test['Credit_Amount'] = df_test['Credit_Amount'].replace('$',np.nan).astype(float)\n",
    "\n",
    "df_train['Loan_Annuity'] = df_train['Loan_Annuity'].replace('#VALUE!',np.nan)\n",
    "df_test['Loan_Annuity'] = df_test['Loan_Annuity'].replace('#VALUE!',np.nan)\n",
    "\n",
    "df_train['Loan_Annuity'] = df_train['Loan_Annuity'].replace('$',np.nan).astype(float)\n",
    "df_test['Loan_Annuity'] = df_test['Loan_Annuity'].replace('$',np.nan).astype(float)\n",
    "\n",
    "df_train['Population_Region_Relative'] = df_train['Population_Region_Relative'].replace('@',np.nan)\n",
    "df_test['Population_Region_Relative'] = df_test['Population_Region_Relative'].replace('@',np.nan)\n",
    "\n",
    "df_train['Population_Region_Relative'] = df_train['Population_Region_Relative'].replace('#',np.nan).astype(float)\n",
    "df_test['Population_Region_Relative'] = df_test['Population_Region_Relative'].replace('#',np.nan).astype(float)\n",
    "\n",
    "df_train['Score_Source_3'] = df_train['Score_Source_3'].replace('&',np.nan).astype(float)\n",
    "df_test['Score_Source_3'] = df_test['Score_Source_3'].replace('#',np.nan).astype(float)\n",
    "\n",
    "df_test['Score_Source_2'] = df_test['Score_Source_3'].replace('#',np.nan).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Client_Gender'] = df_train['Client_Gender'].replace('XNA','Male')\n",
    "df_test['Client_Gender'] = df_test['Client_Gender'].replace('XNA','Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Client_Marital_Status'] = df_test['Client_Marital_Status'].replace('Unknown','M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Age_Days'] = df_train['Age_Days'].replace('x',np.nan).astype(float)\n",
    "df_test['Age_Days'] = df_test['Age_Days'].replace('x',np.nan).astype(float)\n",
    "\n",
    "df_train['Employed_Days'] = df_train['Employed_Days'].replace('x',np.nan).astype(float)\n",
    "df_test['Employed_Days'] = df_test['Employed_Days'].replace('x',np.nan).astype(float)\n",
    "\n",
    "df_train['Registration_Days'] = df_train['Registration_Days'].replace('x',np.nan).astype(float)\n",
    "df_test['Registration_Days'] = df_test['Registration_Days'].replace('x',np.nan).astype(float)\n",
    "\n",
    "df_train['ID_Days'] = df_train['ID_Days'].replace('x',np.nan).astype(float)\n",
    "df_test['ID_Days'] = df_test['ID_Days'].replace('x',np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=[]\n",
    "num=[]\n",
    "for i in df_train.loc[:, ~df_train.columns. isin(['ID'])]:\n",
    "    if df_train[i].dtype==\"object\":\n",
    "        cat.append(i)\n",
    "    else:\n",
    "        num.append(i)\n",
    "       \n",
    "print('Catogorical Variables : \\n*****************\\n', cat) \n",
    "print('\\nNumerical Variables : \\n*****************\\n', num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_train.isnull().sum()[df_train.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_train.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_test.isnull().sum()[df_test.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_test.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Education'].isnull().sum()\n",
    "nmste=df_test['Client_Education'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Education: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing Missing values\n",
    "df_train.groupby('Client_Occupation',dropna=False)['Client_Education'].agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Department with mode value of group by \"Client_Education\"\n",
    "df_train['Client_Education'] = df_train.groupby(['Client_Occupation'],dropna=False)['Client_Education'].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "df_test['Client_Education'] = df_test.groupby(['Client_Occupation'],dropna=False)['Client_Education'].apply(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Education'].isnull().sum()\n",
    "nmste=df_test['Client_Education'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Education: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Occupation'].isnull().sum()\n",
    "nmste=df_test['Client_Occupation'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Occupation: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing Missing values\n",
    "df_train.groupby('Client_Education',dropna=False)['Client_Occupation'].agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Department with mode value of group by \"Client_Education\"\n",
    "df_train['Client_Occupation'] = df_train.groupby(['Client_Education'],dropna=False)['Client_Occupation'].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "df_test['Client_Occupation'] = df_test.groupby(['Client_Education'],dropna=False)['Client_Occupation'].apply(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Occupation'].isnull().sum()\n",
    "nmste=df_test['Client_Occupation'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Occupation: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Income'].isnull().sum()\n",
    "nmste=df_test['Client_Income'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Income: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Client_Income'] = df_train['Client_Income'].fillna(df_train.groupby(by=['Client_Education','Client_Occupation'])['Client_Income'].transform('median'))\n",
    "df_test['Client_Income'] = df_test['Client_Income'].fillna(df_test.groupby(by=['Client_Education','Client_Occupation'])['Client_Income'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Income'].isnull().sum()\n",
    "nmste=df_test['Client_Income'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Income: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Child_Count'].isnull().sum()\n",
    "nmste=df_test['Child_Count'].isnull().sum()\n",
    "print(f'Number of missing values in Child_Count: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Child_Count'] = df_train['Child_Count'].fillna(df_train.groupby(by=['Client_Marital_Status','Client_Family_Members'],dropna=False)['Child_Count'].transform('median'))\n",
    "df_test['Child_Count'] = df_test['Child_Count'].fillna(df_test.groupby(by=['Client_Marital_Status','Client_Family_Members'],dropna=False)['Child_Count'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Child_Count'].isnull().sum()\n",
    "nmste=df_test['Child_Count'].isnull().sum()\n",
    "print(f'Number of missing values in Child_Count: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Child_Count'].fillna(16,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Child_Count'].isnull().sum()\n",
    "nmste=df_test['Child_Count'].isnull().sum()\n",
    "print(f'Number of missing values in Child_Count: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Marital_Status'].isnull().sum()\n",
    "nmste=df_test['Client_Marital_Status'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Marital_Status: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing Missing values\n",
    "df_train.groupby('Client_Family_Members',dropna=False)['Client_Marital_Status'].agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Client_Marital_Status'] = df_train.groupby(['Client_Family_Members'],dropna=False)['Client_Marital_Status'].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "df_test['Client_Marital_Status'] = df_test.groupby(['Client_Family_Members'],dropna=False)['Client_Marital_Status'].apply(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Marital_Status'].isnull().sum()\n",
    "nmste=df_test['Client_Marital_Status'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Marital_Status: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Family_Members'].isnull().sum()\n",
    "nmste=df_test['Client_Family_Members'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Family_Members: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Client_Family_Members'] = df_train['Client_Family_Members'].fillna(df_train.groupby(by=['Client_Marital_Status','Child_Count'],dropna=False)['Client_Family_Members'].transform('median'))\n",
    "df_test['Client_Family_Members'] = df_test['Client_Family_Members'].fillna(df_test.groupby(by=['Client_Marital_Status','Child_Count'],dropna=False)['Client_Family_Members'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Client_Family_Members'].isnull().sum()\n",
    "nmste=df_test['Client_Family_Members'].isnull().sum()\n",
    "print(f'Number of missing values in Client_Family_Members: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Client_Family_Members'].fillna(20,inplace=True)\n",
    "df_test['Client_Family_Members'].fillna(5,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Own_House_Age'].isnull().sum()\n",
    "nmste=df_test['Own_House_Age'].isnull().sum()\n",
    "print(f'Number of missing values in Own_House_Age: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Own_House_Age'] = df_train['Own_House_Age'].fillna(df_train.groupby(by=['House_Own'],dropna=False)['Own_House_Age'].transform('median'))\n",
    "df_test['Own_House_Age'] = df_test['Own_House_Age'].fillna(df_test.groupby(by=['House_Own'],dropna=False)['Own_House_Age'].transform('median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Own_House_Age'].isnull().sum()\n",
    "nmste=df_test['Own_House_Age'].isnull().sum()\n",
    "print(f'Number of missing values in Own_House_Age: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_1'].isnull().sum()\n",
    "nmste=df_test['Score_Source_1'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_1: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Score_Source_1'] = df_train['Score_Source_1'].fillna(df_train['Score_Source_1'].mean())\n",
    "df_test['Score_Source_1'] = df_test['Score_Source_1'].fillna(df_train['Score_Source_1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_1'].isnull().sum()\n",
    "nmste=df_test['Score_Source_1'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_1: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Social_Circle_Default'].isnull().sum()\n",
    "nmste=df_test['Social_Circle_Default'].isnull().sum()\n",
    "print(f'Number of missing values in Social_Circle_Default: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Social_Circle_Default'] = df_train['Social_Circle_Default'].fillna(df_train['Social_Circle_Default'].mean())\n",
    "df_test['Social_Circle_Default'] = df_test['Social_Circle_Default'].fillna(df_train['Social_Circle_Default'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Social_Circle_Default'].isnull().sum()\n",
    "nmste=df_test['Social_Circle_Default'].isnull().sum()\n",
    "print(f'Number of missing values in Social_Circle_Default: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_3'].isnull().sum()\n",
    "nmste=df_test['Score_Source_3'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_3: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Score_Source_3'] = df_train['Score_Source_3'].fillna(df_train['Score_Source_3'].mean())\n",
    "df_test['Score_Source_3'] = df_test['Score_Source_3'].fillna(df_train['Score_Source_3'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_3'].isnull().sum()\n",
    "nmste=df_test['Score_Source_3'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_3: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_2'].isnull().sum()\n",
    "nmste=df_test['Score_Source_2'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_2: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Score_Source_2'] = df_train['Score_Source_2'].fillna(df_train['Score_Source_2'].mean())\n",
    "df_test['Score_Source_2'] = df_test['Score_Source_2'].fillna(df_train['Score_Source_2'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Score_Source_2'].isnull().sum()\n",
    "nmste=df_test['Score_Source_2'].isnull().sum()\n",
    "print(f'Number of missing values in Score_Source_2: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Credit_Bureau'].isnull().sum()\n",
    "nmste=df_test['Credit_Bureau'].isnull().sum()\n",
    "print(f'Number of missing values in Credit_Bureau: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['Credit_Bureau'] = df_train['Credit_Bureau'].fillna(df_train['Credit_Bureau'].median())\n",
    "df_test['Credit_Bureau'] = df_test['Credit_Bureau'].fillna(df_train['Credit_Bureau'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['Credit_Bureau'].isnull().sum()\n",
    "nmste=df_test['Credit_Bureau'].isnull().sum()\n",
    "print(f'Number of missing values in Credit_Bureau: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['ID_Days'].isnull().sum()\n",
    "nmste=df_test['ID_Days'].isnull().sum()\n",
    "print(f'Number of missing values in ID_Days: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the Age with group by \"WorkExprience\" and \"EductaionField\"\n",
    "df_train['ID_Days'] = df_train['ID_Days'].fillna(df_train['ID_Days'].median())\n",
    "df_test['ID_Days'] = df_test['ID_Days'].fillna(df_train['ID_Days'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmstr=df_train['ID_Days'].isnull().sum()\n",
    "nmste=df_test['ID_Days'].isnull().sum()\n",
    "print(f'Number of missing values in ID_Days: \\n Train\\t:\\t{nmstr}\\n Test\\t:\\t{nmste}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train['Population_Region_Relative'] = df_train['Population_Region_Relative'].fillna(df_train['Population_Region_Relative'].mean())\n",
    "df_test['Population_Region_Relative'] = df_test['Population_Region_Relative'].fillna(df_train['Population_Region_Relative'].mean())\n",
    "\n",
    "df_train['Phone_Change'] = df_train['Phone_Change'].fillna(df_train['Phone_Change'].median())\n",
    "df_test['Phone_Change'] = df_test['Phone_Change'].fillna(df_train['Phone_Change'].median())\n",
    "                                                         \n",
    "df_train['Employed_Days'] = df_train['Employed_Days'].fillna(df_train['Employed_Days'].median())\n",
    "df_test['Employed_Days'] = df_test['Employed_Days'].fillna(df_train['Employed_Days'].median())\n",
    "                                                         \n",
    "df_train['Registration_Days'] = df_train['Registration_Days'].fillna(df_train['Registration_Days'].median())\n",
    "df_test['Registration_Days'] = df_test['Registration_Days'].fillna(df_train['Registration_Days'].median())\n",
    "\n",
    "df_train['Age_Days'] = df_train['Age_Days'].fillna(df_train['Age_Days'].median())\n",
    "df_test['Age_Days'] = df_test['Age_Days'].fillna(df_train['Age_Days'].median())\n",
    "\n",
    "df_train['Application_Process_Day'] = df_train['Application_Process_Day'].fillna(df_train['Application_Process_Day'].median())\n",
    "df_test['Application_Process_Day'] = df_test['Application_Process_Day'].fillna(df_train['Application_Process_Day'].median())\n",
    "\n",
    "df_train['Application_Process_Hour'] = df_train['Application_Process_Hour'].fillna(df_train['Application_Process_Hour'].median())\n",
    "df_test['Application_Process_Hour'] = df_test['Application_Process_Hour'].fillna(df_train['Application_Process_Hour'].median())\n",
    "\n",
    "df_train['Credit_Amount'] = df_train['Credit_Amount'].fillna(df_train['Credit_Amount'].mean())\n",
    "df_test['Credit_Amount'] = df_test['Credit_Amount'].fillna(df_train['Credit_Amount'].mean())\n",
    "\n",
    "df_train['Loan_Annuity'] = df_train['Loan_Annuity'].fillna(df_train['Loan_Annuity'].median())\n",
    "df_test['Loan_Annuity'] = df_test['Loan_Annuity'].fillna(df_train['Loan_Annuity'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Cleint_City_Rating'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Car_Owned'].fillna(df_train['Car_Owned'].mode()[0],inplace=True)\n",
    "df_test['Car_Owned'].fillna(df_test['Car_Owned'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['House_Own'].fillna(df_train['House_Own'].mode()[0],inplace=True)\n",
    "df_test['House_Own'].fillna(df_test['House_Own'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Bike_Owned'].fillna(df_train['Bike_Owned'].mode()[0],inplace=True)\n",
    "df_test['Bike_Owned'].fillna(df_test['Bike_Owned'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Active_Loan'].fillna(df_train['Active_Loan'].mode()[0],inplace=True)\n",
    "df_test['Active_Loan'].fillna(df_test['Active_Loan'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Cleint_City_Rating'].fillna(df_train['Cleint_City_Rating'].mode()[0],inplace=True)\n",
    "df_test['Cleint_City_Rating'].fillna(df_test['Cleint_City_Rating'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Accompany_Client'].fillna(df_train['Accompany_Client'].mode()[0],inplace=True)\n",
    "df_test['Accompany_Client'].fillna(df_test['Accompany_Client'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Client_Income_Type'].fillna(df_train['Client_Income_Type'].mode()[0],inplace=True)\n",
    "df_test['Client_Income_Type'].fillna(df_test['Client_Income_Type'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Loan_Contract_Type'].fillna(df_train['Loan_Contract_Type'].mode()[0],inplace=True)\n",
    "df_test['Loan_Contract_Type'].fillna(df_test['Loan_Contract_Type'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Client_Housing_Type'].fillna(df_train['Client_Housing_Type'].mode()[0],inplace=True)\n",
    "df_test['Client_Housing_Type'].fillna(df_test['Client_Housing_Type'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Type_Organization'].fillna(df_train['Type_Organization'].mode()[0],inplace=True)\n",
    "df_test['Type_Organization'].fillna(df_test['Type_Organization'].mode()[0],inplace=True)\n",
    "\n",
    "df_train['Client_Gender'].fillna(df_train['Client_Gender'].mode()[0],inplace=True)\n",
    "df_test['Client_Gender'].fillna(df_test['Client_Gender'].mode()[0],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_train.isnull().sum()[df_train.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_train.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing value\n",
    "print('****Missing Values in the Dataset****')\n",
    "print('*************************************')\n",
    "msv = df_test.isnull().sum()[df_test.isnull().sum()>0]\n",
    "if msv.empty:\n",
    "    print('There is no missing values in the data.') \n",
    "else:\n",
    "    for i in range(msv.count()):\n",
    "        print('{} Missing values in {} which is {}% of total data'.format(msv[i],msv.index[i],round(((msv[i]/df_train.shape[0])*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Client_Income','Credit_Amount','Loan_Annuity','Age_Days','Employed_Days','Registration_Days','ID_Days','Phone_Change']\n",
    "fig, axis=plt.subplots(nrows=8,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "fig.tight_layout()\n",
    "\n",
    "i=0\n",
    "for cols in columns:\n",
    "    sns.boxplot(df_train[cols],ax=axis[i]);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Score_Source_1','Score_Source_2','Score_Source_3','Social_Circle_Default','Population_Region_Relative']\n",
    "fig, axis=plt.subplots(nrows=5,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "fig.tight_layout()\n",
    "\n",
    "i=0\n",
    "for cols in columns:\n",
    "    sns.boxplot(df_train[cols],ax=axis[i]);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to identify the outliers\n",
    "def remove_outlier(col):\n",
    "    sorted(col)\n",
    "    Q1,Q3=np.percentile(col,[25,75])\n",
    "    IQR=Q3-Q1\n",
    "    lower_range= Q1-(1.5 * IQR)\n",
    "    upper_range= Q3+(1.5 * IQR)\n",
    "    return lower_range, upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Client_Income','Credit_Amount','Loan_Annuity','Employed_Days','Registration_Days','Phone_Change']\n",
    "#Fix the outliers \n",
    "for column in df_train[columns].columns:\n",
    "    lr,ur=remove_outlier(df_train[column])\n",
    "    df_train[column]=np.where(df_train[column]>ur,ur,df_train[column])\n",
    "    df_train[column]=np.where(df_train[column]<lr,lr,df_train[column])\n",
    "    \n",
    "    df_test[column]=np.where(df_test[column]>ur,ur,df_test[column])\n",
    "    df_test[column]=np.where(df_test[column]<lr,lr,df_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Client_Income','Credit_Amount','Loan_Annuity','Employed_Days','Registration_Days','Phone_Change']\n",
    "fig, axis=plt.subplots(nrows=6,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "fig.tight_layout()\n",
    "\n",
    "i=0\n",
    "for cols in columns:\n",
    "    sns.boxplot(df_train[cols],ax=axis[i]);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Score_Source_1','Score_Source_2','Score_Source_3','Social_Circle_Default','Population_Region_Relative']\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Score_Source_1','Score_Source_2','Score_Source_3','Social_Circle_Default','Population_Region_Relative']\n",
    "#Fix the outliers \n",
    "for column in df_train[columns].columns:\n",
    "    lr,ur=remove_outlier(df_train[column])\n",
    "    df_train[column]=np.where(df_train[column]>ur,ur,df_train[column])\n",
    "    df_train[column]=np.where(df_train[column]<lr,lr,df_train[column])\n",
    "\n",
    "for column in df_test[columns].columns:\n",
    "    lr,ur=remove_outlier(df_test[column])\n",
    "    df_test[column]=np.where(df_test[column]>ur,ur,df_test[column])\n",
    "    df_test[column]=np.where(df_test[column]<lr,lr,df_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Score_Source_1','Score_Source_2','Score_Source_3','Social_Circle_Default','Population_Region_Relative']\n",
    "fig, axis=plt.subplots(nrows=5,ncols=1)\n",
    "fig.set_size_inches(10,10)\n",
    "fig.tight_layout()\n",
    "\n",
    "i=0\n",
    "for cols in columns:\n",
    "    sns.boxplot(df_train[cols],ax=axis[i]);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['Age_Days']=round(df_train['Age_Days']/365,2)\n",
    "# df_train['Employed_Days']=round(df_train['Employed_Days']/365,2)\n",
    "# df_train['Registration_Days']=round(df_train['Registration_Days']/365,2)\n",
    "# df_train['ID_Days']=round(df_train['ID_Days']/365,2)\n",
    "\n",
    "# df_test['Age_Days']=round(df_test['Age_Days']/365,2)\n",
    "# df_test['Employed_Days']=round(df_test['Employed_Days']/365,2)\n",
    "# df_test['Registration_Days']=round(df_test['Registration_Days']/365,2)\n",
    "# df_test['ID_Days']=round(df_test['ID_Days']/365,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Age_Days','Employed_Days','Registration_Days','ID_Days']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in df_train.columns: \n",
    "#     if df_train[feature].dtype == 'object': \n",
    "#         df_train[feature] = pd.Categorical(df_train[feature]).codes\n",
    "\n",
    "        \n",
    "# for feature in df_test.columns: \n",
    "#     if df_test[feature].dtype == 'object': \n",
    "#         df_test[feature] = pd.Categorical(df_test[feature]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map - Relationalship analysis\n",
    "plt.figure(figsize=(35,10))\n",
    "sns.heatmap(df_train.corr(),annot=True,mask=np.triu(df_train.corr(),+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_dc=df_train.drop(['Child_Count','Accompany_Client','Client_Education','Client_Housing_Type','Population_Region_Relative','Age_Days','Employed_Days','Registration_Days','ID_Days','Own_House_Age','Client_Family_Members','Cleint_City_Rating','Application_Process_Day','Application_Process_Hour','Client_Contact_Work_Tag','Type_Organization','Score_Source_1','Score_Source_2','Score_Source_3'],axis=1)\n",
    "# df_train_dc.head()\n",
    "# # df_train.drop(['Default','ID',tvar], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['Default',tvar], axis=1)\n",
    "y = df_train[tvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# oversample = SMOTE(sampling_strategy='minority')\n",
    "# X, y = oversample.fit_resample(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.30,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = X_test['ID']\n",
    "temp_df = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop('ID',axis=1,inplace=True)\n",
    "X_test.drop('ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in X_train.columns: \n",
    "    if X_train[feature].dtype == 'object': \n",
    "        X_train[feature] = pd.Categorical(X_train[feature]).codes\n",
    "\n",
    "for feature in X_test.columns: \n",
    "    if X_test[feature].dtype == 'object': \n",
    "        X_test[feature] = pd.Categorical(X_test[feature]).codes\n",
    "                \n",
    "for feature in df_test.columns: \n",
    "    if df_test[feature].dtype == 'object': \n",
    "        df_test[feature] = pd.Categorical(df_test[feature]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data using feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_train = scaler.fit_transform(X_train)\n",
    "X_train_sc = pd.DataFrame(sc_train, index=X_train.index, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_test = scaler.transform(X_test)\n",
    "X_test_sc = pd.DataFrame(sc_test, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv_test = scaler.fit_transform(df_test)\n",
    "X_testv_sc = pd.DataFrame(scv_test, index=df_test.index, columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='gini',max_features=18)\n",
    "rf.fit(X_train_sc,y_train)\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(rf.feature_importances_*100,index=X_train.columns).sort_values(by=0,ascending=False)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(x[0],x.index,palette='rainbow')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.xlabel('Feature Importance in %')\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_py = rf.predict(X_train_sc)\n",
    "rf_test_py = rf.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train,rf_train_py))\n",
    "print(classification_report(y_train,rf_train_py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rf_test_py))\n",
    "print(classification_report(y_test,rf_test_py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncl = MLPClassifier(hidden_layer_sizes=1000,max_iter=2500, random_state=1, tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncl.fit(X_train_sc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train = nncl.predict(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train,nn_train))\n",
    "print(classification_report(y_train,nn_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test = nncl.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,nn_test))\n",
    "print(classification_report(y_test,nn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [100,150,250], #1000,700\n",
    "#     'max_iter': [100,250,300], #700\n",
    "#     'activation':['logistic','relu'],\n",
    "#     'solver': ['adam','sgd'],\n",
    "#     'tol': [0.001], #0.01\n",
    "#     'random_state':[0,1] #1\n",
    "# }\n",
    "\n",
    "# nncl = MLPClassifier()\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = nncl, param_grid = param_grid, cv = 3)\n",
    "\n",
    "# grid_search.fit(X_train_sc,y_train)\n",
    "# grid_search.best_params_\n",
    "# nn_model = grid_search.best_estimator_\n",
    "# nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_train_1 = nn_model.predict(X_train_sc)\n",
    "# nn_test_1 = nn_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(y_test,nn_test_1))\n",
    "# print(classification_report(y_test,nn_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.drop(['y_test','rf_test','nn_test'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df['id']=test_ids\n",
    "temp_df['y_test']=y_test\n",
    "temp_df['rf_test']=rf_test_py\n",
    "temp_df['nn_test']=nn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Car_Owned',\t'Bike_Owned',\t'Active_Loan',\t'House_Own',\t'Child_Count',\t'Client_Marital_Status',\t'Client_Gender',\t'Loan_Contract_Type',\t'Own_House_Age',\t'Mobile_Tag',\t'Homephone_Tag',\t'Workphone_Working',\t'Client_Occupation',\t'Client_Family_Members',\t'Cleint_City_Rating',\t'Application_Process_Day',\t'Application_Process_Hour',\t'Client_Permanent_Match_Tag',\t'Client_Contact_Work_Tag',\t'Type_Organization',]\n",
    "for cols in columns:\n",
    "    print((temp_df.groupby('y_test')[cols].agg('count') - temp_df.groupby('nn_test')[cols].agg('count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.groupby('nn_test')['Car_Owned'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Car_Owned',\t'Bike_Owned',\t'Active_Loan',\t'House_Own',\t'Child_Count',\t'Client_Marital_Status',\t'Client_Gender',\t'Loan_Contract_Type',\t'Own_House_Age',\t'Mobile_Tag',\t'Homephone_Tag',\t'Workphone_Working',\t'Client_Occupation',\t'Client_Family_Members',\t'Cleint_City_Rating',\t'Application_Process_Day',\t'Application_Process_Hour',\t'Client_Permanent_Match_Tag',\t'Client_Contact_Work_Tag',\t'Type_Organization',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axis=plt.subplots(nrows=20,ncols=2)\n",
    "fig.set_size_inches(10,50)\n",
    "fig.tight_layout()\n",
    "\n",
    "i=0\n",
    "for cols in columns:\n",
    "    sns.countplot(y=temp_df[cols],hue=temp_df['y_test'],ax=axis[i][0]);\n",
    "    sns.countplot(y=temp_df[cols],hue=temp_df['nn_test'],ax=axis[i][1]);\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv('./Ana_nn.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testv_sc.drop('Default',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p = nncl.predict(X_testv_sc.drop('ID',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop('Default',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Default']= test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['ID','Default']].to_csv('./Hack_submission_2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
